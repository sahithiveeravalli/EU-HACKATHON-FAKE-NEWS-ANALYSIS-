{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "[['mars', 'is', 'the', 'fourth', 'planet', 'in', 'our', 'solar', 'system', '.'], ['it', 'is', 'second-smallest', 'planet', 'in', 'the', 'solar', 'system', 'after', 'mercury', '.'], ['saturn', 'is', 'yellow', 'planet', '.']]\n",
      "{'.': 0, 'fourth': 1, 'in': 2, 'is': 3, 'mars': 4, 'our': 5, 'planet': 6, 'solar': 7, 'system': 8, 'the': 9, 'after': 10, 'it': 11, 'mercury': 12, 'second-smallest': 13, 'saturn': 14, 'yellow': 15}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(0, 1), (2, 1), (3, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(0, 1), (3, 1), (6, 1), (14, 1), (15, 1)]]\n",
      "[['fourth', 0.53], ['in', 0.2], ['mars', 0.53], ['our', 0.53], ['solar', 0.2], ['system', 0.2], ['the', 0.2]]\n",
      "[['in', 0.17], ['solar', 0.17], ['system', 0.17], ['the', 0.17], ['after', 0.47], ['it', 0.47], ['mercury', 0.47], ['second-smallest', 0.47]]\n",
      "[['saturn', 0.71], ['yellow', 0.71]]\n",
      "Number of documents: 1\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "Number of documents: 1\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "0.78813386\n",
      "Average similarity float: 0.2627112865447998\n",
      "Average similarity percentage: 26.27112865447998\n",
      "Average similarity rounded percentage: 26\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "avg: 0.2627112865447998\n",
      "\n",
      "Total_avg 0.2627112865447998\n",
      "percentage_of_Total_avg 26.27112865447998 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "file_docs = []\n",
    "with open ('DemoFile1.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "print(\"Number of documents:\",len(file_docs))\n",
    "\n",
    "\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)]\n",
    "            for text in file_docs]\n",
    "print (gen_docs)\n",
    "\n",
    "\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n",
    "\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file2_docs = []\n",
    "with open ('DemoFile3.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
    "\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "\n",
    "\n",
    "file2_docs = []\n",
    "with open ('DemoFile3.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
    "\n",
    "\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)\n",
    "\n",
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
    "\n",
    "avg_sims = [] # array of averages\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "        # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # print (document_number, document_similarity)\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "        # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # print average of similarity for each query doc\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')    \n",
    "        # add average values into array\n",
    "        avg_sims.append(avg)  \n",
    "   # calculate total average\n",
    "        total_avg = np.sum(avg_sims,dtype=np.float)\n",
    "        print()\n",
    "        if total_avg >=100:\n",
    "          print(\"Total_avg = 100\")\n",
    "        else:\n",
    "             print(\"Total_avg\",total_avg)\n",
    "             print(\"percentage_of_Total_avg\",total_avg*100,\"%\")    \n",
    "    # round the value and multiply by 100 to format it as percentage\n",
    "        percentage_of_similarity = round(float(total_avg) * 100)\n",
    "    # if percentage is greater than 100\n",
    "    # that means documents are almost same\n",
    "        if  percentage_of_similarity >= 100:\n",
    "          percentage_of_similarity = 100\n",
    "          print(percentage_of_similarity,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 3\n",
      "\n",
      "\n",
      "gen_docs = [['mars', 'is', 'the', 'fourth', 'planet', 'in', 'our', 'solar', 'system', '.'], ['it', 'is', 'second-smallest', 'planet', 'in', 'the', 'solar', 'system', 'after', 'mercury', '.'], ['saturn', 'is', 'yellow', 'planet', '.']]\n",
      "\n",
      "\n",
      "token_id= {'.': 0, 'fourth': 1, 'in': 2, 'is': 3, 'mars': 4, 'our': 5, 'planet': 6, 'solar': 7, 'system': 8, 'the': 9, 'after': 10, 'it': 11, 'mercury': 12, 'second-smallest': 13, 'saturn': 14, 'yellow': 15}\n",
      "\n",
      "\n",
      "corpus= [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(0, 1), (2, 1), (3, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(0, 1), (3, 1), (6, 1), (14, 1), (15, 1)]]\n",
      "\n",
      "\n",
      "tf_idf= [['fourth', 0.53], ['in', 0.2], ['mars', 0.53], ['our', 0.53], ['solar', 0.2], ['system', 0.2], ['the', 0.2]]\n",
      "tf_idf= [['in', 0.17], ['solar', 0.17], ['system', 0.17], ['the', 0.17], ['after', 0.47], ['it', 0.47], ['mercury', 0.47], ['second-smallest', 0.47]]\n",
      "tf_idf= [['saturn', 0.71], ['yellow', 0.71]]\n",
      "\n",
      "\n",
      "Number of documents: 1\n",
      "\n",
      "\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "\n",
      "\n",
      "Number of documents: 1\n",
      "\n",
      "\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "\n",
      "\n",
      "sum_of_sims= 0.78813386\n",
      "\n",
      "\n",
      "Average similarity float: 0.2627112865447998\n",
      "Average similarity percentage: 26.27112865447998\n",
      "Average similarity rounded percentage: 26\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "avg: 0.2627112865447998\n",
      "\n",
      "Total_avg 0.2627112865447998\n",
      "percentage_of_Total_avg 26.27112865447998 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "file_docs = []\n",
    "with open ('DemoFile1.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "print(\"Number of documents:\",len(file_docs))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)]\n",
    "            for text in file_docs]\n",
    "print (\"gen_docs =\",gen_docs)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(\"token_id=\",dictionary.token2id)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(\"corpus=\",corpus)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print('tf_idf=',[[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file2_docs = []\n",
    "with open ('DemoFile3.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "print(\"\\n\")\n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
    "\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "file2_docs = []\n",
    "with open ('DemoFile3.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "print(\"Number of documents:\",len(file2_docs))\n",
    "print(\"\\n\")\n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
    "\n",
    "\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "print(\"\\n\")\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print('sum_of_sims=',sum_of_sims)\n",
    "print(\"\\n\")\n",
    "\n",
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
    "\n",
    "avg_sims = [] # array of averages\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "        # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # print (document_number, document_similarity)\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "        # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # print average of similarity for each query doc\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')    \n",
    "        # add average values into array\n",
    "        avg_sims.append(avg)  \n",
    "   # calculate total average\n",
    "        total_avg = np.sum(avg_sims,dtype=np.float)\n",
    "        print()\n",
    "        if total_avg >=100:\n",
    "          print(\"Total_avg = 100\")\n",
    "        else:\n",
    "             print(\"Total_avg\",total_avg)\n",
    "             print(\"percentage_of_Total_avg\",total_avg*100,\"%\")    \n",
    "    # round the value and multiply by 100 to format it as percentage\n",
    "        percentage_of_similarity = round(float(total_avg) * 100)\n",
    "    # if percentage is greater than 100\n",
    "    # that means documents are almost same\n",
    "        if  percentage_of_similarity >= 100:\n",
    "          percentage_of_similarity = 100\n",
    "          print(percentage_of_similarity,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
